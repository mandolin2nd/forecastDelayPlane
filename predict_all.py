import os
import json
import requests
import streamlit as st
import pandas as pd
import numpy as np
from dotenv import load_dotenv
from joblib import load

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
from matplotlib import font_manager, rcParams

# .env íŒŒì¼ì´ ìˆë‹¤ë©´ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
# ì´ ì½”ë“œëŠ” ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œ .env íŒŒì¼ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•¨ì´ë©°,
# Azure App Serviceì™€ ê°™ì€ ë°°í¬ í™˜ê²½ì—ì„œëŠ” ì„œë¹„ìŠ¤ì˜ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì„ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.
load_dotenv()


# =========================
# 0) LLM (Azure OpenAI: gpt-4o-mini) ì„¤ì • - í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ
# =========================
# ë³´ì•ˆì„ ìœ„í•´ ë¯¼ê°í•œ ì •ë³´ëŠ” ì½”ë“œì— ì§ì ‘ ì‘ì„±í•˜ì§€ ì•Šê³  í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
#
# [ë¡œì»¬ ê°œë°œ]
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— .env íŒŒì¼ì„ ë§Œë“¤ê³  ì•„ë˜ ë‚´ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”. (ì´ íŒŒì¼ì€ .gitignoreì— í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤)
# AOAI_ENDPOINT="YOUR_AZURE_OPENAI_ENDPOINT"
# AOAI_API_KEY="YOUR_AZURE_OPENAI_KEY"
AOAI_ENDPOINT    = os.getenv("AOAI_ENDPOINT")
AOAI_DEPLOYMENT  = os.getenv("AOAI_DEPLOYMENT", "gpt-4o-mini")
AOAI_API_KEY     = os.getenv("AOAI_API_KEY")
AOAI_API_VERSION = os.getenv("AOAI_API_VERSION", "2024-08-01-preview")

# --- ì¶”ê°€: í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ---
# Azure App Serviceì— ë°°í¬ ì‹œ, ì•„ë˜ ë³€ìˆ˜ë“¤ì´ ì„¤ì •ë˜ì§€ ì•Šìœ¼ë©´ ì•±ì´ ì‹œì‘ë˜ì§€ ì•Šê³  ëª…í™•í•œ ì˜¤ë¥˜ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
# ì´ë ‡ê²Œ í•˜ë©´ ë””ë²„ê¹…ì´ í›¨ì”¬ ì‰¬ì›Œì§‘ë‹ˆë‹¤.
missing_vars = []
if not AOAI_ENDPOINT:
    missing_vars.append("AOAI_ENDPOINT")
if not AOAI_API_KEY:
    missing_vars.append("AOAI_API_KEY")

if missing_vars:
    st.error(f"ì˜¤ë¥˜: í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {', '.join(missing_vars)}")
    st.info("Azure Portal > App Service > 'êµ¬ì„±' > 'ì‘ìš© í”„ë¡œê·¸ë¨ ì„¤ì •'ì—ì„œ í•´ë‹¹ ë³€ìˆ˜ë“¤ì´ ì˜¬ë°”ë¥´ê²Œ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. ì´ë¦„ì— ì˜¤íƒ€ê°€ ì—†ëŠ”ì§€ ë‹¤ì‹œ í•œë²ˆ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.")
    st.stop()

# =========================
# 1) Matplotlib í•œê¸€ í°íŠ¸
# =========================
font_manager.fontManager.addfont("fonts/NanumGothic-Regular.ttf")
font_manager.fontManager.addfont("fonts/NanumGothic-Bold.ttf")
rcParams["font.family"] = "NanumGothic"
rcParams["axes.unicode_minus"] = False


# =========================
# 2) ìƒìˆ˜ ë° ëª¨ë¸ ë¡œë“œ
# =========================
FEATURE_COLS = ["í’ì†(m/s)", "í’í–¥(deg)", "ì‹œì •(m)", "ê°•ìˆ˜ëŸ‰(mm)", "ìˆœê°„í’ì†(m/s)"]
DEFAULT_MODEL = "./model/flight.joblib"


# =========================
# 3) ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =========================
@st.cache_resource
def load_model(path: str):
    """í•™ìŠµëœ scikit-learn íŒŒì´í”„ë¼ì¸(.joblib) ë¡œë“œ"""
    return load(path)

def build_vector_from_inputs(input_map: dict, feature_cols: list[str]) -> tuple[list | None, str | None]:
    """st.text_input ë§µì—ì„œ ì˜ˆì¸¡ìš© ë²¡í„° ìƒì„±. ì—ëŸ¬ ì‹œ ë©”ì‹œì§€ ë°˜í™˜."""
    vals = []
    provided_count = 0
    for col in feature_cols:
        if col in input_map:
            raw = (input_map[col] or "").strip()
            if raw == "":
                vals.append(None)
            else:
                try:
                    vals.append(float(raw))
                    provided_count += 1
                except (ValueError, TypeError):
                    return None, f"'{col}'ì— ìœ íš¨í•œ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”."
        else:
            # ì„ íƒë˜ì§€ ì•Šì€ ë³€ìˆ˜ëŠ” ê²°ì¸¡ì¹˜ë¡œ ì „ë‹¬
            vals.append(None)
    if provided_count == 0:
        return None, "ìµœì†Œ 1ê°œ ì´ìƒì˜ ë³€ìˆ˜ì— ê°’ì„ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤."
    return vals, None

def predict_one(pipe, row: list[float]) -> dict:
    """ë‹¨ì¼ ì…ë ¥ì— ëŒ€í•´ ì˜ˆì¸¡ í™•ë¥ /ë ˆì´ë¸” ë°˜í™˜"""
    X = pd.DataFrame([row], columns=FEATURE_COLS)
    proba = pipe.predict_proba(X)[0]
    return {
        "label": pipe.classes_[np.argmax(proba)],
        "proba": {cls: float(p) for cls, p in zip(pipe.classes_, proba)}
    }

def to_table_row(name: str, res: dict) -> dict:
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‘œì˜ í•œ í–‰ìœ¼ë¡œ ë³€í™˜"""
    row = {"êµ¬ë¶„": name, "ì˜ˆì¸¡": res["label"]}
    for k, v in res["proba"].items():
        row[f"{k}(%)"] = v * 100
    return row

def build_prompt_json(r1: dict, r2: dict, r3: dict, user_prompt: str) -> dict:
    """gpt-4o-miniì— ë„˜ê¸¸ ë©”ì‹œì§€ ë°”ë”” êµ¬ì„±(ê°„ê²° JSON + ì‚¬ìš©ì ì§€ì‹œë¬¸)"""
    data = {
        "horizons": [
            {"name": "1h", "label": r1["label"], "proba": r1["proba"]},
            {"name": "2h", "label": r2["label"], "proba": r2["proba"]},
            {"name": "3h", "label": r3["label"], "proba": r3["proba"]},
        ]
    }
    system_msg = (
        "ë‹¹ì‹ ì€ í•­ê³µí¸ ì´ë¥™ ì§€ì—°ì´ë‚˜, ì·¨ì†Œ ê°€ëŠ¥ì„±ì„ ì•ˆë‚´í•˜ëŠ” í•œêµ­ì–´ ë¹„ì„œì…ë‹ˆë‹¤."
        "1) ì¶œë°œ/ì§€ì—°/ì·¨ì†Œ í™•ë¥ ì„ ê³ ë ¤í•´ì„œ ìŠ¹ê°ì—ê²Œ ì§€ì—°ì´ë‚˜ ì·¨ì†Œ ê°€ëŠ¥ì„±ì´ ìˆ«ìê°€ ì•„ë‹Œ í‘œí˜„ìœ¼ë¡œ ì•Œë ¤ì¤˜."
        "2) ê³¼ë„í•œ í™•ì‹  í‘œí˜„ì€ í”¼í•˜ê³  í™•ë¥  ê¸°ë°˜ì˜ ì¡°ì‹¬ìŠ¤ëŸ¬ìš´ ì–´ì¡°ë¥¼ ìœ ì§€í•´ì£¼ì„¸ìš”."
        "3) í•­ê³µê¸° ì¶œë°œì´ë¼ëŠ” í‘œí˜„ì„ í•œì¤„ ê¶Œê³ ì— ëª…í™•í•˜ê²Œ ì£¼ì–´ë¡œ í‘œì‹œ"
        "4) ì§€ì—°ì´ë‚˜ ì·¨ì†Œ í™•ë¥ ì— ëŒ€í•´ì„œëŠ” ê¸°ìƒë•Œë¬¸ì´ë¼ëŠ” ì ì„ ëª…í™•í•˜ê²Œ ì•Œë ¤ì¤˜"
        "5) ì¦ê±°ìš´ ì—¬í–‰ì´ ë˜ê¸¸ ê¸°ì›í•˜ëŠ” í•œì¤„ ì¶”ê°€"
    )
    user_msg = user_prompt + "\n\n" + "ì°¸ê³  ë°ì´í„°:\n" + f"DATA={json.dumps(data, ensure_ascii=False)}"
    return {
        "temperature": 0.3,
        "max_tokens": 400,
        "messages": [
            {"role": "system", "content": system_msg},
            {"role": "user", "content": user_msg},
        ],
    }

def call_chat_completions(body: dict) -> dict:
    """Azure OpenAI Chat Completions API í˜¸ì¶œ"""
    url = f"{AOAI_ENDPOINT}/openai/deployments/{AOAI_DEPLOYMENT}/chat/completions?api-version={AOAI_API_VERSION}"
    headers = {"Content-Type": "application/json", "api-key": AOAI_API_KEY}
    resp = requests.post(url, headers=headers, json=body, timeout=40)
    resp.raise_for_status()
    return resp.json()


# =========================
# 4) ëª¨ë¸ ë¡œë“œ ë° ì—ëŸ¬ ì²˜ë¦¬
# =========================
pipe = None
try:
    if not os.path.exists(DEFAULT_MODEL):
        st.error(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DEFAULT_MODEL}")
    else:
        pipe = load_model(DEFAULT_MODEL)
except Exception as e:
    st.error(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")

if pipe is None:
    st.stop()


# =========================
# 5) UI êµ¬ì„± ë° ì˜ˆì¸¡ ì‹¤í–‰
# =========================
st.title("âœˆï¸ í•­ê³µí¸ ìƒíƒœ ì˜ˆì¸¡ + LLM ìš”ì•½")
st.caption("ê³µí†µ 'ì…ë ¥ ë³€ìˆ˜ ì„ íƒ' â†’ ì‹œê°„ëŒ€ë³„(1/2/3ì‹œê°„í›„) ì…ë ¥ â†’ í•œ ë²ˆì— ì˜ˆì¸¡ ë° LLM ì¡°ì–¸ ìš”ì²­")

# ----- ê³µí†µ: ì…ë ¥ ë³€ìˆ˜ ì„ íƒ -----
with st.container():
    st.markdown("### ğŸ”§ ì…ë ¥ ë³€ìˆ˜ ì„ íƒ (ê³µí†µ)")
    selected_features = st.multiselect(
        "ì˜ˆì¸¡ì— ì‚¬ìš©í•  ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.",
        options=FEATURE_COLS,
        default=FEATURE_COLS,
        help="ì„ íƒëœ ë³€ìˆ˜ë§Œ ì•„ë˜ 1/2/3ì‹œê°„í›„ ì…ë ¥ ì˜ì—­ì— ë…¸ì¶œë©ë‹ˆë‹¤. ë¯¸ì„ íƒ/ë¯¸ì…ë ¥ ë³€ìˆ˜ëŠ” ê²°ì¸¡ìœ¼ë¡œ ì „ë‹¬ë˜ì–´ ì¤‘ì•™ê°’ìœ¼ë¡œ ë³´ê°„ë©ë‹ˆë‹¤."
    )
    if not selected_features:
        st.info("ìµœì†Œ 1ê°œ ì´ìƒì˜ ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
    st.markdown("---")

# ----- ì…ë ¥ ì˜ì—­: 1/2/3ì‹œê°„í›„ -----
col1, col2, col3 = st.columns(3)

def input_section(col, horizon_label, features_to_show):
    with col:
        st.markdown(f"### â± {horizon_label} ê°’ ì…ë ¥")
        st.caption("ì„ íƒí•œ ë³€ìˆ˜ë§Œ ì…ë ¥í•˜ì„¸ìš”.")
        inputs = {}
        if not features_to_show:
            st.write("ìœ„ì—ì„œ ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
        for feat in features_to_show:
            ph = {
                "í’ì†(m/s)": "ì˜ˆ: 6.5", "í’í–¥(deg)": "ì˜ˆ: 270", "ì‹œì •(m)": "ì˜ˆ: 9000",
                "ê°•ìˆ˜ëŸ‰(mm)": "ì˜ˆ: 1.2", "ìˆœê°„í’ì†(m/s)": "ì˜ˆ: 10.3",
            }.get(feat, "")
            inputs[feat] = st.text_input(f"{feat}", value="", placeholder=ph, key=f"{horizon_label}_{feat}")
    return inputs

inputs1 = input_section(col1, "1ì‹œê°„í›„", selected_features)
inputs2 = input_section(col2, "2ì‹œê°„í›„", selected_features)
inputs3 = input_section(col3, "3ì‹œê°„í›„", selected_features)

st.divider()

# ----- LLM í”„ë¡¬í”„íŠ¸ ì…ë ¥ (ì„ íƒ) -----
st.markdown("### ğŸ§¾ LLM í”„ë¡¬í”„íŠ¸")
default_prompt = (
    "ë‹¤ìŒ í™•ë¥ ì„ ë°”íƒ•ìœ¼ë¡œ ë¹„í–‰ê¸° ì´ë¥™ ì˜ˆìƒ ì•ˆë‚´ë¬¸ì„ í•œêµ­ì–´ë¡œ 3~4ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì¤˜. "
    "ì¶œë°œ/ì§€ì—°/ì·¨ì†Œ í™•ë¥ ì„ ê³ ë ¤í•´ì„œ ìŠ¹ê°ì—ê²Œ ì¶œë°œ ì§€ì—°ì´ë‚˜ ì·¨ì†Œ ê°€ëŠ¥ì„±ì´ ì–¼ë§Œí¼ì¸ì§€ë¥¼ ì•Œë ¤ì£¼ê³ , ì¦ê±°ìš´ ì—¬í–‰ì´ ë˜ë¼ëŠ” ì¢‹ì€ ì–˜ê¸°ë„ í•œì¤„ ë‚¨ê²¨ì¤˜. "
    "ê³¼ë„í•œ í™•ì‹  í‘œí˜„ì€ í”¼í•˜ê³  í™•ë¥  ê¸°ë°˜ì˜ ì¡°ì‹¬ìŠ¤ëŸ¬ìš´ ì–´ì¡°ë¥¼ ìœ ì§€í•´ì¤˜."
)
user_prompt = st.text_area("í”„ë¡¬í”„íŠ¸", value=default_prompt, height=120, help="gpt-4o-miniì— ì „ë‹¬í•  ì§€ì‹œì‚¬í•­")

# ----- ë‹¨ì¼ í†µí•© ë²„íŠ¼ -----
if st.button("ğŸš€ ì˜ˆì¸¡ ë° AI ì¡°ì–¸ ìš”ì²­", disabled=not selected_features):
    row_1h, err1 = build_vector_from_inputs(inputs1, FEATURE_COLS)
    row_2h, err2 = build_vector_from_inputs(inputs2, FEATURE_COLS)
    row_3h, err3 = build_vector_from_inputs(inputs3, FEATURE_COLS)

    has_err = any((err1, err2, err3))
    if has_err:
        for e in (err1, err2, err3):
            if e: st.warning(e)
    else:
        with st.spinner("ëª¨ë¸ ì˜ˆì¸¡ ë° AI ì¡°ì–¸ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            res_1h = predict_one(pipe, row_1h)
            res_2h = predict_one(pipe, row_2h)
            res_3h = predict_one(pipe, row_3h)

            table_df = pd.DataFrame([
                to_table_row("1ì‹œê°„", res_1h), to_table_row("2ì‹œê°„", res_2h), to_table_row("3ì‹œê°„", res_3h),
            ])
            st.subheader("ì˜ˆì¸¡ ìš”ì•½í‘œ")
            st.dataframe(table_df, use_container_width=True)

            labels = ["ì¶œë°œ", "ì§€ì—°", "ì·¨ì†Œ"]
            vals_1h = [table_df.loc[0, f"{lb}(%)"] for lb in labels]
            vals_2h = [table_df.loc[1, f"{lb}(%)"] for lb in labels]
            vals_3h = [table_df.loc[2, f"{lb}(%)"] for lb in labels]

            x = np.arange(len(labels)); width = 0.25
            st.subheader("1h/2h/3h í™•ë¥  ë¹„êµ (í•œ ì¥ ì°¨íŠ¸)")
            fig, ax = plt.subplots()
            bars1 = ax.bar(x - width, vals_1h, width, label="1h")
            bars2 = ax.bar(x,         vals_2h, width, label="2h")
            bars3 = ax.bar(x + width, vals_3h, width, label="3h")
            ax.set_xticks(x, labels); ax.set_ylabel("í™•ë¥ (%)")
            ax.set_title("ì¶œë°œ/ì§€ì—°/ì·¨ì†Œë³„ 1hÂ·2hÂ·3h í™•ë¥  ë¹„êµ"); ax.legend()
            for bars in (bars1, bars2, bars3):
                for rect in bars:
                    h = rect.get_height()
                    ax.text(rect.get_x()+rect.get_width()/2, h+0.5, f"{h:.1f}", ha="center", va="bottom", size=9)
            st.pyplot(fig)
            st.divider()

            st.subheader("LLM ì„¤ëª… (gpt-4o-mini)")
            try:
                body = build_prompt_json(res_1h, res_2h, res_3h, user_prompt)
                data = call_chat_completions(body)
                msg = (data.get("choices", [{}])[0].get("message", {}).get("content", "")) or ""
                if not msg:
                    st.warning("LLM ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì›ë¬¸(JSON)ì„ ì•„ë˜ì— í‘œì‹œí•©ë‹ˆë‹¤.")
                    st.code(json.dumps(data, ensure_ascii=False, indent=2), language="json")
                else:
                    st.success("âœ… AI ì¡°ì–¸")
                    st.write(msg)
            except Exception as e:
                st.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                st.info("AOAI_ENDPOINT / AOAI_DEPLOYMENT / AOAI_API_KEY / AOAI_API_VERSION ê°’ì„ í™•ì¸í•˜ì„¸ìš”.")


# =========================
# 6) í‘¸í„°
# =========================
st.divider()
st.caption("ğŸ§  ëª¨ë¸: RandomForest(class_weight='balanced') | ì „ì²˜ë¦¬: SimpleImputer(median) | í°íŠ¸: NanumGothic")
